{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ÏúÑÌï¥Î¨ºÌíà ÏóëÏä§Î†àÏù¥ Í≤ÄÏ∂ú ÌîÑÎ°úÍ∑∏Îû®.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juko626/yolov5_Dangerous_object_images_detection_220207/blob/main/%EC%9C%84%ED%95%B4%EB%AC%BC%ED%92%88_%EC%97%91%EC%8A%A4%EB%A0%88%EC%9D%B4_%EA%B2%80%EC%B6%9C_%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATTF5zOqc9gR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f8201e6-c07c-4673-c8a9-81e63599fa60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/ff\n",
        "!mkdir /content/ff2\n",
        "!mkdir /ngrok2\n"
      ],
      "metadata": {
        "id": "zk4TYO4bdFiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ÏãúÏó∞"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdNQUyJSPdkR",
        "outputId": "7a81b8cf-5364-4264-8508-b0f414ebab7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ÏãúÏó∞\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ìè¥Îçî, ÌååÏùº ÏÇ≠Ï†ú \n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "dir_path = \"/content/ff/exp\"\n",
        "\n",
        "if os.path.exists(dir_path):\n",
        "    shutil.rmtree(dir_path)\n",
        "\n",
        "\n",
        "dir_path = \"/content/ff2\"\n",
        "\n",
        "if os.path.exists(dir_path):\n",
        "    shutil.rmtree(dir_path)\n",
        "\n",
        "!mkdir /content/ff2"
      ],
      "metadata": {
        "id": "PGpau_hZ2VuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi nest-asyncio pyngrok uvicorn\n",
        "!pip install jinja2\n",
        "!pip install aiofiles\n",
        "!pip install python-multipart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxnxzEaPdprS",
        "outputId": "444c4365-125b-4a94-c7e0-1c3d7e087bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.70.0-py3-none-any.whl (51 kB)\n",
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 10 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 20 kB 35.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 30 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 40 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 51 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51 kB 526 kB/s \n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (1.5.4)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 745 kB 24.4 MB/s \n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.16.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 54 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting starlette==0.16.0\n",
            "  Downloading starlette-0.16.0-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61 kB 205 kB/s \n",
            "\u001b[?25hCollecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10.1 MB 69.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from starlette==0.16.0->fastapi) (3.10.0.2)\n",
            "Collecting anyio<4,>=3.0.0\n",
            "  Downloading anyio-3.4.0-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.0.0->starlette==0.16.0->fastapi) (2.10)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (3.13)\n",
            "Collecting asgiref>=3.4.0\n",
            "  Downloading asgiref-3.4.1-py3-none-any.whl (25 kB)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 54 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn) (7.1.2)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19006 sha256=28c79a348eee53605267739df433fa5eb6d9420c8e47cbff48d1340a505e6a3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: sniffio, anyio, starlette, pydantic, h11, asgiref, uvicorn, pyngrok, fastapi\n",
            "Successfully installed anyio-3.4.0 asgiref-3.4.1 fastapi-0.70.0 h11-0.12.0 pydantic-1.8.2 pyngrok-5.1.0 sniffio-1.2.0 starlette-0.16.0 uvicorn-0.16.0\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2) (2.0.1)\n",
            "Collecting aiofiles\n",
            "  Downloading aiofiles-0.8.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: aiofiles\n",
            "Successfully installed aiofiles-0.8.0\n",
            "Collecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from python-multipart) (1.15.0)\n",
            "Building wheels for collected packages: python-multipart\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=0be0966a63cec44efaef9f21abe2857b24452453a9b1824049ec59e7e1a88c88\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\n",
            "Successfully built python-multipart\n",
            "Installing collected packages: python-multipart\n",
            "Successfully installed python-multipart-0.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 21z2BmFzf3AmcEyafrxkkJwnIXv_7w4nXnunmsbF2ES1ski93"
      ],
      "metadata": {
        "id": "BwV4UesI3jBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "\n",
        "%cd /content/yolov5/\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "brp-hIivfNXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "from fastapi import FastAPI\n",
        "from fastapi import FastAPI, File, UploadFile, Request, Form, File, UploadFile\n",
        "from fastapi import FastAPI, status\n",
        "from fastapi.responses import HTMLResponse\n",
        "from typing import List\n",
        "import os\n",
        "import uvicorn\n",
        "from fastapi import APIRouter\n",
        "from fastapi.staticfiles import StaticFiles\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "from fastapi.templating import Jinja2Templates\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import torch\n",
        "import shutil"
      ],
      "metadata": {
        "id": "YpyY0RiRdzmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path='/content/drive/MyDrive/ÏãúÏó∞/l50best.pt')"
      ],
      "metadata": {
        "id": "ihLZKEGQd2K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ÏÉàÎ°ú ÎßåÎì†Í±∞ ÏúÑÏóê ÏôÑÎ£å\n",
        "from typing import List\n",
        "\n",
        "from fastapi import FastAPI, File, UploadFile\n",
        "from fastapi.responses import HTMLResponse\n",
        "from fastapi.responses import FileResponse\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image\n",
        "%cd '/content/ff2'\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "\n",
        "@app.post(\"/uploadfiles/\")\n",
        "async def root(file: UploadFile = File(...)):\n",
        "    with open(f'{file.filename}',\"wb\") as buffer:\n",
        "        shutil.copyfileobj(file.file, buffer)\n",
        "        time.sleep(20)  # 2Ï¥à Í∏∞Îã§Î¶º\n",
        "        main(opt)\n",
        "        time.sleep(25)\n",
        "        images=glob(\"/content/ff/exp/*.png\")\n",
        "        voice()\n",
        "        \n",
        "        return FileResponse(images[-1])\n",
        "        \n",
        "       \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "#{\"file_name\": file.filename},\n",
        "@app.get(\"/\")\n",
        "async def main():\n",
        "    content = '''\n",
        "\n",
        "\n",
        "\n",
        "   <html>\n",
        "     <head>\n",
        "      <meta charset=\"UTF-8\" />\n",
        "       <meta name=\"viewport\" content=\"width=240, initial-scale=1.0\" /> \n",
        "       <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\" />\n",
        "        <title>ÏóëÏä§Î†àÏù¥ Ìú¥Î®º ÏóêÎü¨ Í∞êÏÜå ÌîÑÎ°úÏ†ùÌä∏</title>\n",
        "         </head> \n",
        "         <body>\n",
        "         \n",
        "    \n",
        "\n",
        "    <form action=\"/uploadfiles/\" enctype=\"multipart/form-data\" method=\"post\">\n",
        "          <input name=\"file\" type=\"file\" multiple id=\"image\" accept=\"image/*\" onchange=\"setThumbnail(event);\" multiple/>\n",
        "           <div id=\"image_container\"></div>\n",
        "            <script> function setThumbnail(event) {\n",
        "                 for (var image of event.target.files) {\n",
        "                      var reader = new FileReader(); \n",
        "                      reader.onload = function(event) { \n",
        "                          var img = document.createElement(\"img\");\n",
        "                           img.setAttribute(\"src\", event.target.result);\n",
        "                            document.querySelector(\"div#image_container\").appendChild(img);\n",
        "                             }; console.log(image); reader.readAsDataURL(image); } } </script> \n",
        "                             <input type=\"submit\">\n",
        "                             \n",
        "          </body>\n",
        "           </html>\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return HTMLResponse(content=content)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#dfdsfdssdfdsfd\n",
        "# YOLOv5 üöÄ by Ultralytics, GPL-3.0 license\n",
        "\"\"\"\n",
        "Run inference on images, videos, directories, streams, etc.\n",
        "\n",
        "Usage:\n",
        "    $ python path/to/detect.py --weights yolov5s.pt --source 0  # webcam\n",
        "                                                             img.jpg  # image\n",
        "                                                             vid.mp4  # video\n",
        "                                                             path/  # directory\n",
        "                                                             path/*.jpg  # glob\n",
        "                                                             'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n",
        "                                                             'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from models.common import DetectMultiBackend\n",
        "from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "#Ï∂îÍ∞Ä\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from glob import glob\n",
        "import yaml\n",
        "import os\n",
        "from PIL import Image\n",
        "import urllib.request\n",
        "#FILE = Path(__file__).resolve()\n",
        "#ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
        "ROOT='/content/drive/MyDrive/ÏãúÏó∞'\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
        "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def run(weights=ROOT / 'l50best.pt',  # model.pt path(s)\n",
        "        source=ROOT/ '/content/ff2/*.png',  # file/dir/URL/glob, 0 for webcam\n",
        "        imgsz=640,  # inference size (pixels)\n",
        "        conf_thres=0.25,  # confidence threshold\n",
        "        iou_thres=0.45,  # NMS IOU threshold\n",
        "        max_det=1000,  # maximum detections per image\n",
        "        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
        "        view_img=True,  # show results\n",
        "        save_txt=True,  # save results to *.txt\n",
        "        save_conf=True,  # save confidences in --save-txt labels\n",
        "        save_crop=False,  # save cropped prediction boxes\n",
        "        nosave=False,  # do not save images/videos\n",
        "        classes=None,  # filter by class: --class 0, or --class 0 2 3\n",
        "        agnostic_nms=False,  # class-agnostic NMS\n",
        "        augment=False,  # augmented inference\n",
        "        visualize=False,  # visualize features\n",
        "        update=False,  # update all models\n",
        "        project=ROOT / '/content/ff',  # save results to project/name\n",
        "        name='exp',  # save results to project/name\n",
        "        exist_ok=True,  # existing project/name ok, do not increment\n",
        "        line_thickness=3,  # bounding box thickness (pixels)\n",
        "        hide_labels=False,  # hide labels\n",
        "\n",
        "        hide_conf=False,  # hide confidences\n",
        "        half=False,  # use FP16 half-precision inference\n",
        "        dnn=False,  # use OpenCV DNN for ONNX inference\n",
        "        ):\n",
        "    source = str(source)\n",
        "    save_img = not nosave and not source.endswith('.txt')  # save inference images\n",
        "    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n",
        "    is_url = source.lower().startswith(('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
        "    webcam = source.isnumeric() or source.endswith('.txt') or (is_url and not is_file)\n",
        "    if is_url and is_file:\n",
        "        source = check_file(source)  # download\n",
        "\n",
        "    # Directories\n",
        "    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n",
        "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
        "\n",
        "    # Load model\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weights, device=device, dnn=dnn)\n",
        "    stride, names, pt, jit, onnx = model.stride, model.names, model.pt, model.jit, model.onnx\n",
        "    imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
        "\n",
        "    # Half\n",
        "    half &= pt and device.type != 'cpu'  # half precision only supported by PyTorch on CUDA\n",
        "    if pt:\n",
        "        model.model.half() if half else model.model.float()\n",
        "\n",
        "    # Dataloader\n",
        "    if webcam:\n",
        "        view_img = check_imshow()\n",
        "        cudnn.benchmark = True  # set True to speed up constant image size inference\n",
        "        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt and not jit)\n",
        "        bs = len(dataset)  # batch_size\n",
        "    else:\n",
        "        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt and not jit)\n",
        "        bs = 1  # batch_size\n",
        "    vid_path, vid_writer = [None] * bs, [None] * bs\n",
        "\n",
        "    # Run inference\n",
        "    # if pt and device.type != 'cpu':\n",
        "        #  model(torch.zeros(1, 3, *imgsz).to(device).type_as(next(model.model.parameters())))  # warmup\n",
        "    dt, seen = [0.0, 0.0, 0.0], 0\n",
        "    for path, im, im0s, vid_cap, s in dataset:\n",
        "        t1 = time_sync()\n",
        "        im = torch.from_numpy(im).to(device)\n",
        "        im = im.half() if half else im.float()  # uint8 to fp16/32\n",
        "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
        "        if len(im.shape) == 3:\n",
        "            im = im[None]  # expand for batch dim\n",
        "        t2 = time_sync()\n",
        "        dt[0] += t2 - t1\n",
        "\n",
        "        # Inference\n",
        "        visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
        "        pred = model(im, augment=augment, visualize=visualize)\n",
        "        t3 = time_sync()\n",
        "        dt[1] += t3 - t2\n",
        "\n",
        "        # NMS\n",
        "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "        dt[2] += time_sync() - t3\n",
        "\n",
        "        # Second-stage classifier (optional)\n",
        "        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n",
        "\n",
        "        # Process predictions\n",
        "        for i, det in enumerate(pred):  # per image\n",
        "            seen += 1\n",
        "            if webcam:  # batch_size >= 1\n",
        "                p, im0, frame = path[i], im0s[i].copy(), dataset.count\n",
        "                s += f'{i}: '\n",
        "            else:\n",
        "                p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)\n",
        "\n",
        "            p = Path(p)  # to Path\n",
        "            save_path = str(save_dir / p.name)  # im.jpg\n",
        "            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt\n",
        "            s += '%gx%g ' % im.shape[2:]  # print string\n",
        "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
        "            imc = im0.copy() if save_crop else im0  # for save_crop\n",
        "            annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
        "            if len(det):\n",
        "                # Rescale boxes from img_size to im0 size\n",
        "                det[:, :4] = scale_coords(im.shape[2:], det[:, :4], im0.shape).round()\n",
        "\n",
        "                # Print results\n",
        "                for c in det[:, -1].unique():\n",
        "                    n = (det[:, -1] == c).sum()  # detections per class\n",
        "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
        "\n",
        "                # Write results\n",
        "                for *xyxy, conf, cls in reversed(det):\n",
        "                    if save_txt:  # Write to file\n",
        "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
        "                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
        "                        with open(txt_path + '.txt', 'a') as f:\n",
        "                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
        "\n",
        "                    if save_img or save_crop or view_img:  # Add bbox to image\n",
        "                        c = int(cls)  # integer class\n",
        "                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
        "                        annotator.box_label(xyxy, label, color=colors(c, True))\n",
        "                        if save_crop:\n",
        "                            save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n",
        "\n",
        "            # Print time (inference-only)\n",
        "            LOGGER.info(f'{s}Done. ({t3 - t2:.3f}s)')\n",
        "\n",
        "            # Stream results\n",
        "            im0 = annotator.result()\n",
        "            if view_img:\n",
        "                cv2_imshow(im0)\n",
        "                #cv2_waitKey(1)  # 1 millisecond\n",
        "                #cv2.destroyAllWindows()\n",
        "\n",
        "            # Save results (image with detections)\n",
        "            if save_img:\n",
        "                if dataset.mode == 'image':\n",
        "                    cv2.imwrite(save_path, im0)\n",
        "                else:  # 'video' or 'stream'\n",
        "                    if vid_path[i] != save_path:  # new video\n",
        "                        vid_path[i] = save_path\n",
        "                        if isinstance(vid_writer[i], cv2.VideoWriter):\n",
        "                            vid_writer[i].release()  # release previous video writer\n",
        "                        if vid_cap:  # video\n",
        "                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
        "                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "                        else:  # stream\n",
        "                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
        "                            save_path += '.mp4'\n",
        "                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
        "                    vid_writer[i].write(im0)\n",
        "\n",
        "    # Print results\n",
        "    t = tuple(x / seen * 1E3 for x in dt)  # speeds per image\n",
        "    #LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}' % t)\n",
        "    if save_txt or save_img:\n",
        "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
        "        LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n",
        "    if update:\n",
        "        strip_optimizer(weights)  # update model (to fix SourceChangeWarning)\n",
        "\n",
        "\n",
        "def parse_opt():\n",
        "    import easydict\n",
        "    opt = easydict.EasyDict({\n",
        " \n",
        "        \"weights\": '/content/drive/MyDrive/ÏãúÏó∞/l50best.pt',\n",
        " \n",
        "        \"conf\": 0.5,\n",
        " \n",
        "        #\"save-txt\": True,\n",
        " \n",
        "        #\"save-conf\": True,\n",
        " \n",
        "        \"source\": \"/content/drive/MyDrive/ÏãúÏó∞/ÌååÏùº/H_8424.20-1000_01_376.png\",\n",
        "\n",
        "        \"--view-img\": True\n",
        "    })\n",
        "\n",
        "# !python /content/drive/MyDrive/Astrophysics/1122_mount/yolov5/detect.py \n",
        "# --weights /content/drive/MyDrive/Astrophysics/1122_mount/yolov5/runs/train/multiple_yolov5s_results/weights/best.pt \n",
        "# --img 416  \n",
        "# --conf 0.5  \n",
        "# --save-txt \n",
        "# --save-conf \n",
        "# --source \"/content/drive/MyDrive/Astrophysics/1122_mount/multiple2-1/train/images/H_8424-20-1000_01_318_png.rf.3a9a8164ddb2baf54ba813b62714c76b.jpg\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path(s)')\n",
        "    # parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob, 0 for webcam')\n",
        "    # parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')\n",
        "    # parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')\n",
        "    # parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')\n",
        "    # parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')\n",
        "    # parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    # parser.add_argument('--view-img', action='store_true', help='show results')\n",
        "    # parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
        "    # parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
        "    # parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')\n",
        "    # parser.add_argument('--nosave', action='store_true', help='do not save images/videos')\n",
        "    # parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')\n",
        "    # parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n",
        "    # parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
        "    # parser.add_argument('--visualize', action='store_true', help='visualize features')\n",
        "    # parser.add_argument('--update', action='store_true', help='update all models')\n",
        "    # parser.add_argument('--project', default=ROOT / 'runs/detect', help='save results to project/name')\n",
        "    # parser.add_argument('--name', default='exp', help='save results to project/name')\n",
        "    # parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
        "    # parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')\n",
        "    # parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')\n",
        "    # parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')\n",
        "    # parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n",
        "    # parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')\n",
        "    # opt = parser.parse_args()\n",
        "    #opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand\n",
        "    #print_args(FILE.stem, opt)\n",
        "    return opt.weights\n",
        "\n",
        "def voice():\n",
        "    yaml_data={'names': ['Axe', 'Chisel', 'Gun', 'HDD', 'HandCuffs', 'Knife', 'Lighter', 'Plier', 'Saw', 'Scissors', 'Screwdriver', 'SmartPhone', 'Spanner', 'SupplymentaryBattery', 'USB']}\n",
        "    txt_list=[]\n",
        "    dir_path = \"/content/ff/exp/labels/\"\n",
        "    for (root, directories, files) in os.walk(dir_path):\n",
        "        for file in files:\n",
        "            if '.txt' in file:\n",
        "                file_path = os.path.join(root, file)\n",
        "                txt_list.append(file_path)\n",
        "    final_text_list1=[] \n",
        "    count=1\n",
        "    for i in txt_list:\n",
        "        file = open(i, \"r\")\n",
        "        strings = file.readlines()\n",
        "        A=(list(strings))\n",
        "        #print(\"#\",count,\"Î≤àÏß∏ Ïù¥ÎØ∏ÏßÄ\")\n",
        "        count+=1\n",
        "        for j in range(len(A)):\n",
        "            #print(yaml_data[\"names\"][int(A[0][0])],\"Ïù¥ ÌôïÏù∏ÎêòÏóàÏäµÎãàÎã§.\")\n",
        "            a=(A[j][-9:-1])\n",
        "            #print(\"Ï†ïÌôïÎèÑÎäî:\",round(float(a)*100,2),\"%ÏûÖÎãàÎã§.\")\n",
        "            # final_text_list.append(yaml_data[\"names\"][int(A[0][0])],\"Ïù¥ ÌôïÏù∏ÎêòÏóàÏäµÎãàÎã§.\")\n",
        "            # final_text_list.append(\"Ï†ïÌôïÎèÑÎäî:\",round(float(a)*100,2),\"%ÏûÖÎãàÎã§.\")\n",
        "            a1=yaml_data[\"names\"][int(A[0][0])]\n",
        "            b1=round(float(a)*100,2)\n",
        "            final_text_list1.append([a1,b1])\n",
        "        file.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    client_id = \"cpnxyta5q5\"\n",
        "    client_secret = \"rRCmUPvNNg7dUSGAV93RpnHNkvUs6gt23OvvPW23\"\n",
        "\n",
        "    text=\"\"\n",
        "    text_1=\"Ï†ïÌôïÎèÑÎäî \"\n",
        "    text_5=\" ÏûÖÎãàÎã§.\"\n",
        "    list_word=[]\n",
        "\n",
        "\n",
        "    for i in yaml_data[\"names\"]:\n",
        "        if i in str(final_text_list1):\n",
        "            list_word.append(i)\n",
        "    \n",
        "\n",
        "    for i in list_word:\n",
        "        text1=\"\"\n",
        "        text1+=i+\" Í∞Ä \"+str(str(final_text_list1).count(i)) +\" Í∞ú Í≤ÄÏ∂ú ÎêòÏóàÏäµÎãàÎã§.\"\n",
        "        text1+=text_1\n",
        "\n",
        "        for j in range(len(final_text_list1)):\n",
        "            if final_text_list1[j][0]==i:\n",
        "            # print(j)\n",
        "            # print(final_text_list1[j][1])\n",
        "                text3=\"\"\n",
        "                text4=\"\"\n",
        "                text4=str(final_text_list1[j][1])\n",
        "                text3=text3+text4+\"%  \"\n",
        "                text5=\"ÏûÖÎãàÎã§.\"\n",
        "                text1+=str(text3)\n",
        "    \n",
        "        text1=text1+text5\n",
        "        text+=text1\n",
        "        print(text1)\n",
        "\n",
        "    encText = urllib.parse.quote(text)\n",
        "    data = \"speaker=nara&volume=0&speed=0&pitch=0&format=mp3&text=\" + encText;\n",
        "    url = \"https://naveropenapi.apigw.ntruss.com/tts-premium/v1/tts\"\n",
        "\n",
        "    request = urllib.request.Request(url)\n",
        "    request.add_header(\"X-NCP-APIGW-API-KEY-ID\",client_id)\n",
        "    request.add_header(\"X-NCP-APIGW-API-KEY\",client_secret)\n",
        "    response = urllib.request.urlopen(request, data=data.encode('utf-8'))\n",
        "    rescode = response.getcode()\n",
        "\n",
        "    if(rescode==200):\n",
        "        print(\"TTS mp3 Ï†ÄÏû•\")\n",
        "        response_body = response.read()\n",
        "        with open('Ï¥¨ÏòÅrkaclf{0}.mp3'.format(i), 'wb') as f:\n",
        "            f.write(response_body)\n",
        "    else:\n",
        "        print(\"Error Code:\" + rescode)  \n",
        "\n",
        "def main(opt):\n",
        "    #check_requirements(exclude=('tensorboard', 'thop'))\n",
        "    run(opt)\n",
        "    #voice()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    opt = parse_opt()\n",
        "    #main(opt)\n",
        "\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL :' , ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, host='0.0.0.0', port=8000)"
      ],
      "metadata": {
        "id": "awL4aiUtevUN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}